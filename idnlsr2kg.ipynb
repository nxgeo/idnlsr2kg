{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbR2alIEoCFJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "!pip install --upgrade spacy[cuda12x]\n",
        "!pip install --no-deps spacy-stanza\n",
        "!pip install --upgrade networkx\n",
        "\n",
        "!sudo apt-get install graphviz graphviz-dev\n",
        "!pip install pygraphviz\n",
        "\n",
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "ellF2rLkqA9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "bd9iLtYoMLuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from collections import defaultdict, namedtuple\n",
        "from itertools import product\n",
        "from operator import attrgetter\n",
        "\n",
        "from networkx import MultiDiGraph\n",
        "from networkx.drawing.nx_agraph import to_agraph\n",
        "from spacy import prefer_gpu\n",
        "from spacy.displacy import render\n",
        "from spacy.tokens import Doc, Span\n",
        "from spacy_stanza import load_pipeline\n",
        "from stanza import download\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "def is_prepositional_nmod(nmod):\n",
        "    return any(token.dep_ == \"case\" for token in nmod.lefts)\n",
        "\n",
        "\n",
        "def expand_noun(noun):\n",
        "    def _expand_noun(_noun):\n",
        "        _noun_phrase = [_noun]\n",
        "        for token in _noun.children:\n",
        "            if (token.pos_ in {\"NOUN\", \"PROPN\", \"ADJ\"}\n",
        "                    and token.dep_ in {\"flat:name\", \"compound\",\n",
        "                                       \"nmod\", \"amod\"}):\n",
        "                if token.dep_ == \"nmod\" and is_prepositional_nmod(token):\n",
        "                    continue\n",
        "                for conjunct in token.conjuncts:\n",
        "                    if conjunct.pos_ in {\"NOUN\", \"PROPN\", \"ADJ\"}:\n",
        "                        _noun_phrase.extend(_expand_noun(conjunct))\n",
        "                        for token_ in conjunct.lefts:\n",
        "                            if token_.dep_ in {\"cc\", \"punct\"}:\n",
        "                                _noun_phrase.append(token_)\n",
        "                _noun_phrase.extend(_expand_noun(token))\n",
        "        return _noun_phrase\n",
        "\n",
        "    noun_phrase = _expand_noun(noun)\n",
        "    noun_phrase.sort(key=attrgetter(\"i\"))\n",
        "    return noun_phrase\n",
        "\n",
        "\n",
        "def extract_args(head_noun):\n",
        "    arguments = [expand_noun(head_noun)]\n",
        "    for conjunct in head_noun.conjuncts:\n",
        "        if conjunct.pos_ in {\"NOUN\", \"PROPN\"}:\n",
        "            arguments.append(expand_noun(conjunct))\n",
        "    return arguments\n",
        "\n",
        "\n",
        "def dist_subjs_mods_to_verb_conjs(verbs):\n",
        "    for verb, details in verbs.items():\n",
        "        head = verbs.get(verb.head)\n",
        "        if head is not None and verb.dep_ == \"conj\":\n",
        "            if not details.get(\"subjects\") and head.get(\"subjects\"):\n",
        "                details[\"subjects\"] = head[\"subjects\"]\n",
        "            if not details.get(\"modifiers\") and head.get(\"modifiers\"):\n",
        "                details[\"modifiers\"] = head[\"modifiers\"]\n",
        "    return None\n",
        "\n",
        "\n",
        "def dist_objs_to_verb_conjs(verbs):\n",
        "    for verb, details in verbs.items():\n",
        "        if objects := details.get(\"objects\"):\n",
        "            for conjunct in verb.conjuncts:\n",
        "                conj = verbs.get(conjunct)\n",
        "                if (conj is not None\n",
        "                        and conjunct.i < verb.i\n",
        "                        and not conj.get(\"objects\")):\n",
        "                    conj[\"objects\"] = objects\n",
        "    return None\n",
        "\n",
        "\n",
        "def handle_xcomp_subjs(verbs):\n",
        "    for verb, details in verbs.items():\n",
        "        head = verbs.get(verb.head)\n",
        "        if (head is not None\n",
        "                and verb.dep_ == \"xcomp\"\n",
        "                and details.get(\"objects\")):\n",
        "            details[\"subjects\"] = head.get(\"objects\") or head[\"subjects\"]\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_svo_triples(sentence):\n",
        "    SVOTriple = namedtuple(\"SVOTriple\", [\"s\", \"v\", \"o\"])\n",
        "    svo_triples = []\n",
        "    verbs = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    for token in sentence:\n",
        "        head = token.head\n",
        "        if token.pos_ == \"VERB\":\n",
        "            verb = verbs[token]\n",
        "            if (token.dep_ in {\"acl\", \"acl:relcl\"}\n",
        "                    and head.pos_ in {\"NOUN\", \"PROPN\"}):\n",
        "                verb[\"subjects\"].extend(extract_args(head))\n",
        "        elif head.pos_ == \"VERB\":\n",
        "            head = verbs[head]\n",
        "            if token.pos_ in {\"NOUN\", \"PROPN\"}:\n",
        "                if token.dep_ in {\"nsubj\", \"nsubj:pass\"}:\n",
        "                    head[\"subjects\"].extend(extract_args(token))\n",
        "                elif token.dep_ == \"obj\":\n",
        "                    head[\"objects\"].extend(extract_args(token))\n",
        "            elif token.dep_ in {\"advmod\", \"aux\"}:\n",
        "                head[\"modifiers\"].append(token)\n",
        "\n",
        "    dist_objs_to_verb_conjs(verbs)\n",
        "    handle_xcomp_subjs(verbs)\n",
        "    dist_subjs_mods_to_verb_conjs(verbs)\n",
        "\n",
        "    for verb, details in verbs.items():\n",
        "        subjects = details.get(\"subjects\")\n",
        "        objects = details.get(\"objects\")\n",
        "        if subjects and objects:\n",
        "            vp = [verb]\n",
        "            if verb_modifiers := details.get(\"modifiers\"):\n",
        "                vp.extend(verb_modifiers)\n",
        "                vp.sort(key=attrgetter(\"i\"))\n",
        "            for s, o in product(subjects, objects):\n",
        "                svo_triples.append(SVOTriple(s=s, v=vp, o=o))\n",
        "\n",
        "    return svo_triples\n",
        "\n",
        "\n",
        "def merge_tokens_into_text(tokens):\n",
        "    text = \"\".join(token.text_with_ws for token in tokens)\n",
        "    return text.lower().rstrip()\n",
        "\n",
        "\n",
        "def collect_svo_triples(doc):\n",
        "    svo_triples = []\n",
        "    for sentence in doc.sents:\n",
        "        for s, v, o in sentence._.svo_triples:\n",
        "            s = merge_tokens_into_text(s)\n",
        "            v = merge_tokens_into_text(v)\n",
        "            o = merge_tokens_into_text(o)\n",
        "            svo_triples.append((s, v, o))\n",
        "    return svo_triples\n",
        "\n",
        "\n",
        "def construct_kg(doc):\n",
        "    svo_triples = set(doc._.svo_triples)\n",
        "    G = MultiDiGraph()\n",
        "    for s, v, o in svo_triples:\n",
        "        G.add_edge(s, o, label=v)\n",
        "    A = to_agraph(G)\n",
        "    A.graph_attr.update(overlap=\"scale\", splines=\"true\")\n",
        "    A.layout(prog=\"sfdp\")\n",
        "    return A\n",
        "\n",
        "\n",
        "stanza_config = {\n",
        "    \"lang\": \"id\",\n",
        "    \"package\": \"default_accurate\",\n",
        "    \"processors\": \"tokenize,mwt,pos,lemma,depparse\",\n",
        "    \"model_dir\": \"/gdrive/MyDrive/stanza_resources\",\n",
        "}\n",
        "\n",
        "download(**stanza_config)\n",
        "\n",
        "prefer_gpu()\n",
        "\n",
        "nlp = load_pipeline(\n",
        "    name=\"id\", use_gpu=True, download_method=None, **stanza_config\n",
        ")\n",
        "\n",
        "Span.set_extension(\"svo_triples\", getter=extract_svo_triples, force=True)\n",
        "Doc.set_extension(\"svo_triples\", getter=collect_svo_triples, force=True)\n",
        "Doc.set_extension(\"kg\", getter=construct_kg, force=True)\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"IDNLSR2KG\", page_icon=\":brain:\")\n",
        "\n",
        "st.title(\":brain: IDNLSR2KG\")\n",
        "st.write(\n",
        "    \"Effortlessly transform software requirements into structured knowledge.\"\n",
        ")\n",
        "\n",
        "dataset = st.file_uploader(\n",
        "    \"Upload software requirement dataset\", type=\"txt\"\n",
        ")\n",
        "\n",
        "if dataset:\n",
        "    with st.spinner(\"Processing...\"):\n",
        "        unique_lines = set()\n",
        "\n",
        "        for line in dataset:\n",
        "            if line := line.decode().strip():\n",
        "                unique_lines.add(line.rstrip(\".\"))\n",
        "\n",
        "        requirements = \". \".join(unique_lines)\n",
        "\n",
        "        doc = nlp(requirements)\n",
        "\n",
        "        st.header(\"Knowledge Graph\")\n",
        "        kg_svg = doc._.kg.draw(format=\"svg\")\n",
        "        st.image(kg_svg.decode())\n",
        "\n",
        "        st.divider()\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            st.subheader(\"Requirement Sentence\")\n",
        "            st.write(sent.text)\n",
        "\n",
        "            st.subheader(\"POS & Dependency\")\n",
        "            dep_svg = render(\n",
        "                sent,\n",
        "                style=\"dep\",\n",
        "                jupyter=False,\n",
        "                options={\"collapse_punct\": False},\n",
        "            )\n",
        "            st.image(dep_svg)\n",
        "\n",
        "            st.subheader(\"Extracted SVO Triples\")\n",
        "            svo_triples = sent._.svo_triples\n",
        "            if not svo_triples:\n",
        "                st.write(\"None\")\n",
        "                st.divider()\n",
        "                continue\n",
        "            for s, v, o in svo_triples:\n",
        "                s = merge_tokens_into_text(s)\n",
        "                v = merge_tokens_into_text(v)\n",
        "                o = merge_tokens_into_text(o)\n",
        "                st.write(f\"({s}, {v}, {o})\")\n",
        "\n",
        "            st.divider()"
      ],
      "metadata": {
        "id": "O7ejd-qrqN7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qO- ifconfig.me"
      ],
      "metadata": {
        "id": "imspZYF7Nc6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "2nJzwcXSMkfW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}